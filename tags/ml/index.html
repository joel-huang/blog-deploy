<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: ml - joelhuang.dev</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="joelhuang.dev"><meta name="msapplication-TileImage" content="https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&amp;v=beta&amp;t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="joelhuang.dev"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="joelhuang.dev"><meta property="og:url" content="https://joelhuang.dev/"><meta property="og:site_name" content="joelhuang.dev"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://joelhuang.dev/img/og_image.png"><meta property="article:author" content="Joel Huang"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://joelhuang.dev/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://joelhuang.dev"},"headline":"joelhuang.dev","image":["https://joelhuang.dev/img/og_image.png"],"author":{"@type":"Person","name":"Joel Huang"},"publisher":{"@type":"Organization","name":"joelhuang.dev","logo":{"@type":"ImageObject","url":"https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&v=beta&t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU"}},"description":""}</script><link rel="icon" href="https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&amp;v=beta&amp;t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&amp;v=beta&amp;t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU" alt="joelhuang.dev" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">ml</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-12-18T08:50:18.000Z" title="18/12/2021, 4:50:18 pm">2021-12-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-03-31T12:33:47.526Z" title="31/03/2024, 8:33:47 pm">2024-03-31</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/18/Blazing-Fast-Pairwise-Cosine-Similarity/">Blazing Fast Pairwise Cosine Similarity</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="I-accidentally-implemented-the-fastest-pairwise-cosine-similarity-function"><a href="#I-accidentally-implemented-the-fastest-pairwise-cosine-similarity-function" class="headerlink" title="I accidentally implemented the fastest pairwise cosine similarity function"></a>I accidentally implemented the fastest pairwise cosine similarity function</h1><p>While searching for a way to efficiently compute pairwise cosine similarity between vectors, I created a simple and efficient implementation using PyTorch. The function runs blazingly fast. It is faster than the popular <code>cosine_similarity</code> function from <code>sklearn</code> and the naive loop-based implementations.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def pairwise_cosine_similarity(tensor: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Args:</span><br><span class="line">        tensor: A tensor of shape (N, D) where N is the number of vectors and D is the dimensionality of the vectors.</span><br><span class="line">    Returns:</span><br><span class="line">        A tensor of shape (N, N) containing the cosine similarity between all pairs of vectors.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    tmm = torch.mm(tensor, tensor.T)</span><br><span class="line">    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)</span><br><span class="line">    denom_mat = torch.mm(denom.T, denom)</span><br><span class="line">    return torch.nan_to_num(tmm / denom_mat)</span><br></pre></td></tr></table></figure>

<h1 id="About-cosine-similarity"><a href="#About-cosine-similarity" class="headerlink" title="About cosine similarity"></a>About cosine similarity</h1><p>Cosine similarity is a intuitive metric to measure similarity between two vectors. It is widely used in vision, recommendation systems, search engines, and natural language processing. The cosine similarity between two vectors is defined as the cosine of the angle between them, ranging from -1 to 1, where 1 means the vectors are identical, -1 means they are opposite, and 0 means they are orthogonal.</p>
<p>Edit (March 2024): Be cautious about using cosine similarity when working with embeddings. Please check out <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.05440.pdf">Is Cosine-Similarity of Embeddings Really About<br>Similarity?</a></p>
<h1 id="The-use-case"><a href="#The-use-case" class="headerlink" title="The use case"></a>The use case</h1><p>We usually want to compute the cosine similarity between all pairs of vectors. This enables do things like searching for similar vectors (similar images), analyzing the structure of the vector space (clustering images). However, with a large matrix, computing the cosine similarity can be computationally expensive.</p>
<p>We often find ourselves having a matrix of shape $(N, D)$ where $N$ is the number of vectors and $D$ is the dimensionality of the vectors. $D$ is also called the embedding size or dimension, which is typically a power of 2, e.g. 64, 512, 1024, 4096, etc.</p>
<h1 id="The-method"><a href="#The-method" class="headerlink" title="The method"></a>The method</h1><p>The method is based on the following formula:</p>
<p>$$\text{sim}(v_i, v_j) &#x3D; \frac{v_i \cdot v_j}{\lVert v_i \rVert \lVert v_j \rVert}$$</p>
<p>Let’s explain what the code does:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmm = torch.mm(tensor, tensor.T)</span><br><span class="line">denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)</span><br><span class="line">denom_mat = torch.mm(denom.T, denom)</span><br><span class="line">return torch.nan_to_num(tmm / denom_mat)</span><br></pre></td></tr></table></figure>
<ol>
<li><p>Numerator matrix: Dot product via matrix multiplication <code>tmm = torch.mm(tensor, tensor.T)</code></p>
<blockquote>
<p>This line computes the matrix multiplication of the input <code>tensor</code> with its transpose <code>tensor.T</code>. The result is a tensor <code>tmm</code> of shape $(N, N)$ where each element $(i, j)$ represents the dot product of vectors $i$ and $j$.</p>
</blockquote>
</li>
<li><p>Denominator values: Norm of each vector <code>denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)</code></p>
<blockquote>
<p>The diagonal of the tensor <code>tmm</code> contains the dot products of each vector with itself. Taking the square root of these values gives the magnitude (or norm) of each vector. The <code>unsqueeze(0)</code> function is used to add an extra dimension to the tensor, changing its shape from $(N,)$ to $(1, N)$.</p>
</blockquote>
</li>
<li><p>Denominator matrix <code>denom_mat = torch.mm(denom.T, denom)</code></p>
<blockquote>
<p>This line computes the outer product of the vector <code>denom</code> with itself, resulting in a matrix <code>denom_mat</code> of shape $(N, N)$. Each element $(i, j)$ of this matrix is the product of the magnitudes of vectors $i$ and $j$.</p>
</blockquote>
</li>
<li><p>NaN removal <code>return torch.nan_to_num(tmm / denom_mat)</code></p>
<blockquote>
<p>Finally, the cosine similarity between each pair of vectors is calculated by dividing the dot product matrix <code>tmm</code> by the matrix <code>denom_mat</code>. The division is element-wise, so each element $(i, j)$ of the resulting matrix represents the cosine similarity between vectors $i$ and $j$. <code>torch.nan_to_num</code> is used to replace any NaN values that might occur during the division with zeros.</p>
</blockquote>
</li>
</ol>
<p>The output is a symmetric matrix where the diagonal elements are all 1 (since the cosine similarity of a vector with itself is always 1), and the off-diagonal elements represent the cosine similarity between different pairs of vectors!</p>
<h1 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h1><p>Versus naive loops, our approach completely outperforms them by several orders of magnitude. Versus <code>sklearn.metrics.pairwise.cosine_similarity</code>, our implementation is 10x faster, and versus a <code>numpy</code> implementation using the exact same logic, our PyTorch code is about 2-3x faster.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Result within 1e-8 of scipy loop: True</span><br><span class="line">Result within 1e-8 of numpy loop: True</span><br><span class="line">Result within 1e-8 of torch loop: True</span><br><span class="line">Result within 1e-8 of sklearn: True</span><br><span class="line">Result within 1e-8 of numpy matrix: True</span><br><span class="line">scipy loop:   142362.0 us</span><br><span class="line">numpy loop:   112752.9 us</span><br><span class="line">torch loop:   83144.2 us</span><br><span class="line">sklearn:      401.8 us</span><br><span class="line">numpy matrix: 136.2 us</span><br><span class="line">✨ ours:      48.6 us</span><br></pre></td></tr></table></figure>
<h2 id="Benchmark-code"><a href="#Benchmark-code" class="headerlink" title="Benchmark code"></a>Benchmark code</h2><p>Here is the benchmark code if anybody wishes to reproduce it:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">import scipy</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def torch_loop_cosine_similarity(tensor, output):</span><br><span class="line">    for i in range(len(tensor)):</span><br><span class="line">        for j in range(len(tensor)):</span><br><span class="line">            output[i, j] = torch.cosine_similarity(</span><br><span class="line">                tensor[i].unsqueeze(0), tensor[j].unsqueeze(0)</span><br><span class="line">            )</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def scipy_loop_cosine_similarity(tensor, output):</span><br><span class="line">    for i in range(len(tensor)):</span><br><span class="line">        for j in range(len(tensor)):</span><br><span class="line">            output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j])</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def numpy_loop_cosine_similarity(tensor, output):</span><br><span class="line">    for i in range(len(tensor)):</span><br><span class="line">        for j in range(len(tensor)):</span><br><span class="line">            output[i, j] = float(</span><br><span class="line">                np.dot(tensor[i], tensor[j])</span><br><span class="line">                / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j]))</span><br><span class="line">            )</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def numpy_matrix_cosine_similarity(tensor):</span><br><span class="line">    tmm = np.matmul(tensor, tensor.T)</span><br><span class="line">    denom = np.sqrt(tmm.diagonal()).unsqueeze(0)</span><br><span class="line">    denom_mat = np.matmul(denom.T, denom)</span><br><span class="line">    return np.nan_to_num(tmm / denom_mat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def torch_matrix_cosine_similarity(tensor):</span><br><span class="line">    tmm = torch.mm(tensor, tensor.T)</span><br><span class="line">    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)</span><br><span class="line">    denom_mat = torch.mm(denom.T, denom)</span><br><span class="line">    return torch.nan_to_num(tmm / denom_mat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    # Create a random data tensor of shape (N=50, D=64)</span><br><span class="line">    data = torch.rand((50, 64))</span><br><span class="line"></span><br><span class="line">    # Create the output tensor</span><br><span class="line">    zeros = torch.zeros((data.shape[0], data.shape[0]))</span><br><span class="line"></span><br><span class="line">    # Compare across different implementations</span><br><span class="line">    sklearn = torch.Tensor(cosine_similarity(data, data))</span><br><span class="line">    npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data))</span><br><span class="line">    npy_loop = numpy_loop_cosine_similarity(data, zeros)</span><br><span class="line">    spy_loop = scipy_loop_cosine_similarity(data, zeros)</span><br><span class="line">    torch_loop = torch_loop_cosine_similarity(data, zeros)</span><br><span class="line">    ours = torch_matrix_cosine_similarity(data)</span><br><span class="line"></span><br><span class="line">    print(f&quot;Result within 1e-8 of scipy loop: &#123;bool(torch.allclose(ours, spy_loop))&#125;&quot;)</span><br><span class="line">    print(f&quot;Result within 1e-8 of numpy loop: &#123;bool(torch.allclose(ours, npy_loop))&#125;&quot;)</span><br><span class="line">    print(f&quot;Result within 1e-8 of torch loop: &#123;bool(torch.allclose(ours, torch_loop))&#125;&quot;)</span><br><span class="line">    print(f&quot;Result within 1e-8 of sklearn: &#123;bool(torch.allclose(ours, sklearn))&#125;&quot;)</span><br><span class="line">    print(</span><br><span class="line">        f&quot;Result within 1e-8 of numpy matrix: &#123;bool(torch.allclose(ours, npy_matrix))&#125;&quot;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    import timeit</span><br><span class="line"></span><br><span class="line">    t0 = timeit.Timer(</span><br><span class="line">        stmt=&quot;torch_loop_cosine_similarity(data, output)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    t1 = timeit.Timer(</span><br><span class="line">        stmt=&quot;cosine_similarity(data, data)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import cosine_similarity&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    t2 = timeit.Timer(</span><br><span class="line">        stmt=&quot;scipy_loop_cosine_similarity(data, output)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    t5 = timeit.Timer(</span><br><span class="line">        stmt=&quot;numpy_loop_cosine_similarity(data, output)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    t3 = timeit.Timer(</span><br><span class="line">        stmt=&quot;numpy_matrix_cosine_similarity(data)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import numpy_matrix_cosine_similarity&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    t4 = timeit.Timer(</span><br><span class="line">        stmt=&quot;torch_matrix_cosine_similarity(data)&quot;,</span><br><span class="line">        setup=&quot;from __main__ import torch_matrix_cosine_similarity&quot;,</span><br><span class="line">        globals=&#123;&quot;data&quot;: data&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(f&quot;scipy loop:   &#123;t2.timeit(100) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br><span class="line">    print(f&quot;numpy loop:   &#123;t5.timeit(100) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br><span class="line">    print(f&quot;torch loop:   &#123;t0.timeit(100) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br><span class="line">    print(f&quot;sklearn:      &#123;t1.timeit(100) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br><span class="line">    print(f&quot;numpy matrix: &#123;t3.timeit(100) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br><span class="line">    print(f&quot;✨ ours:     &#123;t4.timeit(10) / 100 * 1e6:&gt;5.1f&#125; us&quot;)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-12-02T23:10:32.000Z" title="03/12/2021, 7:10:32 am">2021-12-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-03-31T12:35:20.314Z" title="31/03/2024, 8:35:20 pm">2024-03-31</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/03/Creating-Synthetic-Data-via-Neural-Placement/">Creating Synthetic Data via Neural Placement</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Contextual-placement-using-pre-trained-vision-models"><a href="#Contextual-placement-using-pre-trained-vision-models" class="headerlink" title="Contextual placement using pre-trained vision models"></a>Contextual placement using pre-trained vision models</h2><p>Early on in Bifrost’s journey, we took the simplest approach to create object detection scenarios: composite objects randomly on backgrounds. However, as contextual placement became necessary, the approach didn’t scale.</p>
<p>Why contexual? We need to create specialized samples to reinforce distributions and correlations in synthetic datasets. Object detectors trained on synthetic data, for instance, can fail on objects in rare or unrepresented contexts (referring to the environmental pixels <em>surrounding</em> the object), like aircrafts in grassy fields, or large boats in shipyards instead of on water.</p>
<p>We thus built a smarter, but simple, method of determining suitable placement areas in large images, which we endearingly named <em>Mise en Place</em>.</p>
<h2 id="Embedding-image-areas-into-vectors"><a href="#Embedding-image-areas-into-vectors" class="headerlink" title="Embedding image areas into vectors"></a>Embedding image areas into vectors</h2><p>The early layers in trained convolutional neural networks are powerful feature extractors, even on smaller inputs<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Sergey Zagoruyko, & Nikos Komodakis (2015). Learning to Compare Image Patches via Convolutional Neural Networks. arXiv preprint arXiv: Arxiv-1504.03641.
">[1]</span></a></sup>. We can exploit these layers to project small areas of the image into a feature embedding space, in which the embedding vectors have some notion of vector similarity.</p>
<p>Here’s how it works. We first split the original tile into a grid, feeding each individual cell into the initial layers of a pre-trained network (which collectively act as an encoder) and obtaining an embedding vector for that cell:</p>
<div>
    <svg id="full"></svg>
    <svg id="zoomed"></svg>
</div>
<div style="text-align: center;">
    <svg id="encoder"></svg>
    <svg id="embedding"></svg>
</div>


<p>This vector acts as a latent representation of the content of the input cell.</p>
<p>We then compute the cosine similarity between each cell’s embedding vector $(v_i)$, and every other vector $(v_j)$, for every $(j \in S \setminus i)$, $$\text{sim}(v_i, v_j) &#x3D; \frac{v_i \cdot v_j}{\lVert v_i \rVert \lVert v_j \rVert}$$</p>
<p><img src="/mise-en-place/vecsim.svg"></p>
<h2 id="Why-not-just-compare-the-images-directly"><a href="#Why-not-just-compare-the-images-directly" class="headerlink" title="Why not just compare the images directly?"></a>Why not just compare the images directly?</h2><p>Evidently, similar cells produce vectors similar to each other. Now, one may wonder, why bother transforming the image cell into a vector? Why not simply calculate the similarity (or difference) of the input cell directly? Here’s the problem.</p>
<p>Say we’re contrasting two cells (X) and (Y) by taking the sum of the absolute differences between corresponding pixel values:</p>
<p>$$\text{dist}(X, Y) &#x3D; \sum_{i,j} \left\lvert,X_{i,j} - Y_{i,j},\right\rvert$$</p>
<p>For the pairs below, the top pair has a total difference of (951103), and the bottom (490118). What’s happening here?</p>
<div style="text-align: center;">

<p><img src="/mise-en-place/cellsim951103.png"><br><img src="/mise-en-place/cellnotsim490118.png"></p>
</div>

<p>Let’s look at the top row. The two cells seem visually similar, but the position of the diagonal feature doesn’t line up perfectly. This causes the absolute difference between the two images to be large, showin up as white areas in the difference map.</p>
<p>On the other hand, the bottom pair of cells don’t look similar at all, and ar likely from different areas of the original image. But their difference remains low, since pixel value coincidentally match at the same locations, resulting in a lower difference score.</p>
<p>The usefulness of the neural network her is in its <em>translationally-invariant</em> representation of the features in the image, creating embedding vectors that encode the content of each cell, rather than their literal values.</p>
<h2 id="Selecting-matching-vectors"><a href="#Selecting-matching-vectors" class="headerlink" title="Selecting matching vectors"></a>Selecting matching vectors</h2><p>Now that we’ve scored our cells, we’ll want to query a single cell, and search for all the similar cells in the image. For instance, we’ve got this snazzy looking cell of… dirt over here.</p>
<div style="text-align: center;">

<p><img src="/mise-en-place/query.png"></p>
</div>

<p>Since we have this query’s similarity scores for every cell, we can rank the cells, filtering those most similar to it. The naïve approach is to choose the top-(k) similar cells, but the manual task of determining the optimal number of cells remains. An alternative is to let the image speak for itself: we’ll break down the distribution of scores into a Gaussian mixture model<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Duda, R., & Hart, P. (1973). Pattern classification and scene analysis. (Vol. 3) Wiley New York.
">[2]</span></a></sup> and select cells belonging to the <span style="color: #ffa95d;">highest-scoring cluster</span>.</p>
<div style="text-align: center;">

<p><img src="/mise-en-place/gmm.png"><br><img src="/mise-en-place/similar.png"><br><img src="/mise-en-place/squares.png"></p>
</div>

<h2 id="Doing-science"><a href="#Doing-science" class="headerlink" title="Doing science"></a>Doing science</h2><p>We might have managed to produce a visually consistent result, but we’ll also need to design a performance metric, which lets us perform science: tweak parameters and measure how things change. From the final group of similar cells, we can recover a coarse segmentation mask, compared to the original land-cover segmentation mask on the right. Though we’ve managed to match a significant proportion of the cover, we’ve also let too much through.</p>
<div style="text-align: center;">

<p><img src="/mise-en-place/coarse.png"></p>
</div>

<p>To quantify our closeness to the original segmentation mask, we’ll employ the Jaccard index<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="Jaccard, P. (1912). THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE. New Phytologist, 11(2), 37-50.">[3]</span></a></sup>, or intersection-over-union (IoU):</p>
<p>$$J(y, \hat{y}) &#x3D; \frac{\lvert y \cap \hat{y} \rvert}{\lvert y \cup \hat{y} \rvert} &#x3D; 0.447$$</p>
<p>That’s great! Now we’re able to tweak some design choices and observe how close our results are to the ground-truth segmentation.</p>
<div style="text-align: center;">

<p><img src="/mise-en-place/iou_vs_cellsize.png"></p>
</div>

<p>For instance, we can vary the cell size, and track its effect on the Jaccard index. For each cell size, we’d take the average IoU over 5 runs, using a random query cell each time. This hints that the optimal cell size for this particular image is around 35px – if we’re restricting ourselves to a fixed cell size, we can run this experiment over many images and try and determine the best average case.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This was a useful early approach to distribute objects on underrepresented areas. Though this method retains and leverages the raw power of a trained neural network, it’s fast and quick to develop, since no training is involved!</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><script src=https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js></script>
<script src="https://d3js.org/d3.v7.min.js"></script>
<script src="/javascript/mise_en_place.js"></script><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Sergey Zagoruyko, &amp; Nikos Komodakis (2015). Learning to Compare Image Patches via Convolutional Neural Networks. arXiv preprint arXiv: Arxiv-1504.03641.<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Duda, R., &amp; Hart, P. (1973). Pattern classification and scene analysis. (Vol. 3) Wiley New York.<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Jaccard, P. (1912). THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE. New Phytologist, 11(2), 37-50.<a href="#fnref:3" rev="footnote"> ↩</a></span></li></ol></div></div></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&amp;v=beta&amp;t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU" alt="Joel Huang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joel Huang</p><p class="is-size-6 is-block">Product 🤝 AI</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bifrost.ai, Singapore</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://linkedin.com/in/joel-huang" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://linkedin.com/in/joel-huang"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/jowlz_ai"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/joel-huang"><i class="fab fa-github"></i></a></div></div></div><!--!--><!--!--><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/embeddings/"><span class="tag">embeddings</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/meta/"><span class="tag">meta</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ml/"><span class="tag">ml</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/synthetic-data/"><span class="tag">synthetic-data</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-03-17T20:57:07.191Z">2024-03-18</time></p><p class="title"><a href="/2024/03/18/hello-world/">Hello world (again)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-12-18T08:50:18.000Z">2021-12-18</time></p><p class="title"><a href="/2021/12/18/Blazing-Fast-Pairwise-Cosine-Similarity/">Blazing Fast Pairwise Cosine Similarity</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-12-02T23:10:32.000Z">2021-12-03</time></p><p class="title"><a href="/2021/12/03/Creating-Synthetic-Data-via-Neural-Placement/">Creating Synthetic Data via Neural Placement</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://media.licdn.com/dms/image/C5103AQGohlbqg9gtFw/profile-displayphoto-shrink_200_200/0/1576065377685?e=1716422400&amp;v=beta&amp;t=YBWVyzuzshlG2xLCXVqq_IxNfHwf75fOBZGSHWWbMHU" alt="joelhuang.dev" height="28"></a><p class="is-size-7"><span>&copy; 2024 Joel Huang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>