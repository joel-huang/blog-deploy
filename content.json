{"posts":[{"title":"Blazing Fast Pairwise Cosine Similarity","text":"I accidentally implemented the fastest pairwise cosine similarity functionWhile searching for a way to efficiently compute pairwise cosine similarity between vectors, I created a simple and efficient implementation using PyTorch. The function runs blazingly fast. It is faster than the popular cosine_similarity function from sklearn and the naive loop-based implementations. 123456789101112def pairwise_cosine_similarity(tensor: torch.Tensor) -&gt; torch.Tensor: &quot;&quot;&quot; Args: tensor: A tensor of shape (N, D) where N is the number of vectors and D is the dimensionality of the vectors. Returns: A tensor of shape (N, N) containing the cosine similarity between all pairs of vectors. &quot;&quot;&quot; tmm = torch.mm(tensor, tensor.T) denom = torch.sqrt(tmm.diagonal()).unsqueeze(0) denom_mat = torch.mm(denom.T, denom) return torch.nan_to_num(tmm / denom_mat) About cosine similarityCosine similarity is a intuitive metric to measure similarity between two vectors. It is widely used in vision, recommendation systems, search engines, and natural language processing. The cosine similarity between two vectors is defined as the cosine of the angle between them, ranging from -1 to 1, where 1 means the vectors are identical, -1 means they are opposite, and 0 means they are orthogonal. Edit (March 2024): Be cautious about using cosine similarity when working with embeddings. Please check out Is Cosine-Similarity of Embeddings Really AboutSimilarity? The use caseWe usually want to compute the cosine similarity between all pairs of vectors. This enables do things like searching for similar vectors (similar images), analyzing the structure of the vector space (clustering images). However, with a large matrix, computing the cosine similarity can be computationally expensive. We often find ourselves having a matrix of shape $(N, D)$ where $N$ is the number of vectors and $D$ is the dimensionality of the vectors. $D$ is also called the embedding size or dimension, which is typically a power of 2, e.g. 64, 512, 1024, 4096, etc. The methodThe method is based on the following formula: $$\\text{sim}(v_i, v_j) = \\frac{v_i \\cdot v_j}{\\lVert v_i \\rVert \\lVert v_j \\rVert}$$ Let‚Äôs explain what the code does: 1234tmm = torch.mm(tensor, tensor.T)denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)denom_mat = torch.mm(denom.T, denom)return torch.nan_to_num(tmm / denom_mat) Numerator matrix: Dot product via matrix multiplication tmm = torch.mm(tensor, tensor.T) This line computes the matrix multiplication of the input tensor with its transpose tensor.T. The result is a tensor tmm of shape $(N, N)$ where each element $(i, j)$ represents the dot product of vectors $i$ and $j$. Denominator values: Norm of each vector denom = torch.sqrt(tmm.diagonal()).unsqueeze(0) The diagonal of the tensor tmm contains the dot products of each vector with itself. Taking the square root of these values gives the magnitude (or norm) of each vector. The unsqueeze(0) function is used to add an extra dimension to the tensor, changing its shape from $(N,)$ to $(1, N)$. Denominator matrix denom_mat = torch.mm(denom.T, denom) This line computes the outer product of the vector denom with itself, resulting in a matrix denom_mat of shape $(N, N)$. Each element $(i, j)$ of this matrix is the product of the magnitudes of vectors $i$ and $j$. NaN removal return torch.nan_to_num(tmm / denom_mat) Finally, the cosine similarity between each pair of vectors is calculated by dividing the dot product matrix tmm by the matrix denom_mat. The division is element-wise, so each element $(i, j)$ of the resulting matrix represents the cosine similarity between vectors $i$ and $j$. torch.nan_to_num is used to replace any NaN values that might occur during the division with zeros. The output is a symmetric matrix where the diagonal elements are all 1 (since the cosine similarity of a vector with itself is always 1), and the off-diagonal elements represent the cosine similarity between different pairs of vectors! BenchmarkingVersus naive loops, our approach completely outperforms them by several orders of magnitude. Versus sklearn.metrics.pairwise.cosine_similarity, our implementation is 10x faster, and versus a numpy implementation using the exact same logic, our PyTorch code is about 2-3x faster. Results1234567891011Result within 1e-8 of scipy loop: TrueResult within 1e-8 of numpy loop: TrueResult within 1e-8 of torch loop: TrueResult within 1e-8 of sklearn: TrueResult within 1e-8 of numpy matrix: Truescipy loop: 142362.0 usnumpy loop: 112752.9 ustorch loop: 83144.2 ussklearn: 401.8 usnumpy matrix: 136.2 us‚ú® ours: 48.6 us Benchmark codeHere is the benchmark code if anybody wishes to reproduce it: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import torchimport numpy as npimport scipyfrom sklearn.metrics.pairwise import cosine_similaritydef torch_loop_cosine_similarity(tensor, output): for i in range(len(tensor)): for j in range(len(tensor)): output[i, j] = torch.cosine_similarity( tensor[i].unsqueeze(0), tensor[j].unsqueeze(0) ) return outputdef scipy_loop_cosine_similarity(tensor, output): for i in range(len(tensor)): for j in range(len(tensor)): output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j]) return outputdef numpy_loop_cosine_similarity(tensor, output): for i in range(len(tensor)): for j in range(len(tensor)): output[i, j] = float( np.dot(tensor[i], tensor[j]) / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j])) ) return outputdef numpy_matrix_cosine_similarity(tensor): tmm = np.matmul(tensor, tensor.T) denom = np.sqrt(tmm.diagonal()).unsqueeze(0) denom_mat = np.matmul(denom.T, denom) return np.nan_to_num(tmm / denom_mat)def torch_matrix_cosine_similarity(tensor): tmm = torch.mm(tensor, tensor.T) denom = torch.sqrt(tmm.diagonal()).unsqueeze(0) denom_mat = torch.mm(denom.T, denom) return torch.nan_to_num(tmm / denom_mat)if __name__ == &quot;__main__&quot;: # Create a random data tensor of shape (N=50, D=64) data = torch.rand((50, 64)) # Create the output tensor zeros = torch.zeros((data.shape[0], data.shape[0])) # Compare across different implementations sklearn = torch.Tensor(cosine_similarity(data, data)) npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data)) npy_loop = numpy_loop_cosine_similarity(data, zeros) spy_loop = scipy_loop_cosine_similarity(data, zeros) torch_loop = torch_loop_cosine_similarity(data, zeros) ours = torch_matrix_cosine_similarity(data) print(f&quot;Result within 1e-8 of scipy loop: {bool(torch.allclose(ours, spy_loop))}&quot;) print(f&quot;Result within 1e-8 of numpy loop: {bool(torch.allclose(ours, npy_loop))}&quot;) print(f&quot;Result within 1e-8 of torch loop: {bool(torch.allclose(ours, torch_loop))}&quot;) print(f&quot;Result within 1e-8 of sklearn: {bool(torch.allclose(ours, sklearn))}&quot;) print( f&quot;Result within 1e-8 of numpy matrix: {bool(torch.allclose(ours, npy_matrix))}&quot; ) import timeit t0 = timeit.Timer( stmt=&quot;torch_loop_cosine_similarity(data, output)&quot;, setup=&quot;from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;, globals={&quot;data&quot;: data}, ) t1 = timeit.Timer( stmt=&quot;cosine_similarity(data, data)&quot;, setup=&quot;from __main__ import cosine_similarity&quot;, globals={&quot;data&quot;: data}, ) t2 = timeit.Timer( stmt=&quot;scipy_loop_cosine_similarity(data, output)&quot;, setup=&quot;from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;, globals={&quot;data&quot;: data}, ) t5 = timeit.Timer( stmt=&quot;numpy_loop_cosine_similarity(data, output)&quot;, setup=&quot;from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));&quot;, globals={&quot;data&quot;: data}, ) t3 = timeit.Timer( stmt=&quot;numpy_matrix_cosine_similarity(data)&quot;, setup=&quot;from __main__ import numpy_matrix_cosine_similarity&quot;, globals={&quot;data&quot;: data}, ) t4 = timeit.Timer( stmt=&quot;torch_matrix_cosine_similarity(data)&quot;, setup=&quot;from __main__ import torch_matrix_cosine_similarity&quot;, globals={&quot;data&quot;: data}, ) print(f&quot;scipy loop: {t2.timeit(100) / 100 * 1e6:&gt;5.1f} us&quot;) print(f&quot;numpy loop: {t5.timeit(100) / 100 * 1e6:&gt;5.1f} us&quot;) print(f&quot;torch loop: {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us&quot;) print(f&quot;sklearn: {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us&quot;) print(f&quot;numpy matrix: {t3.timeit(100) / 100 * 1e6:&gt;5.1f} us&quot;) print(f&quot;‚ú® ours: {t4.timeit(10) / 100 * 1e6:&gt;5.1f} us&quot;)","link":"/2021/12/18/Blazing-Fast-Pairwise-Cosine-Similarity/"},{"title":"Hello world (again)","text":"I am once again deciding to set up a personal site, domain (while it‚Äôs still available), and blog. I realized I‚Äôve been putting up content on the web as early as 2008/09, and it‚Äôs been a while since I‚Äôve had a place to put up my thoughts and work in a more organized manner. I have a feeling it‚Äôll be a mix of thoughts, musings and ideas centered around tech, AI, product, design, entrepreneurship - but I‚Äôm letting this evolve over time and let‚Äôs see where it goes. Hexo wasn‚Äôt that hard to set up, and this theme was beautiful. The comment system from utteranc.es is pretty good, but it‚Äôll bias comments towards GitHub users. I‚Äôll try this out first.","link":"/2024/03/18/hello-world/"},{"title":"Creating Synthetic Data via Neural Placement","text":"Contextual placement using pre-trained vision modelsEarly on in Bifrost‚Äôs journey, we took the simplest approach to create object detection scenarios: composite objects randomly on backgrounds. However, as contextual placement became necessary, the approach didn‚Äôt scale. Why contexual? We need to create specialized samples to reinforce distributions and correlations in synthetic datasets. Object detectors trained on synthetic data, for instance, can fail on objects in rare or unrepresented contexts (referring to the environmental pixels surrounding the object), like aircrafts in grassy fields, or large boats in shipyards instead of on water. We thus built a smarter, but simple, method of determining suitable placement areas in large images, which we endearingly named Mise en Place. Embedding image areas into vectorsThe early layers in trained convolutional neural networks are powerful feature extractors, even on smaller inputs[1]. We can exploit these layers to project small areas of the image into a feature embedding space, in which the embedding vectors have some notion of vector similarity. Here‚Äôs how it works. We first split the original tile into a grid, feeding each individual cell into the initial layers of a pre-trained network (which collectively act as an encoder) and obtaining an embedding vector for that cell: This vector acts as a latent representation of the content of the input cell. We then compute the cosine similarity between each cell‚Äôs embedding vector $(v_i)$, and every other vector $(v_j)$, for every $(j \\in S \\setminus i)$, $$\\text{sim}(v_i, v_j) = \\frac{v_i \\cdot v_j}{\\lVert v_i \\rVert \\lVert v_j \\rVert}$$ Why not just compare the images directly?Evidently, similar cells produce vectors similar to each other. Now, one may wonder, why bother transforming the image cell into a vector? Why not simply calculate the similarity (or difference) of the input cell directly? Here‚Äôs the problem. Say we‚Äôre contrasting two cells (X) and (Y) by taking the sum of the absolute differences between corresponding pixel values: $$\\text{dist}(X, Y) = \\sum_{i,j} \\left\\lvert,X_{i,j} - Y_{i,j},\\right\\rvert$$ For the pairs below, the top pair has a total difference of (951103), and the bottom (490118). What‚Äôs happening here? Let‚Äôs look at the top row. The two cells seem visually similar, but the position of the diagonal feature doesn‚Äôt line up perfectly. This causes the absolute difference between the two images to be large, showin up as white areas in the difference map. On the other hand, the bottom pair of cells don‚Äôt look similar at all, and ar likely from different areas of the original image. But their difference remains low, since pixel value coincidentally match at the same locations, resulting in a lower difference score. The usefulness of the neural network her is in its translationally-invariant representation of the features in the image, creating embedding vectors that encode the content of each cell, rather than their literal values. Selecting matching vectorsNow that we‚Äôve scored our cells, we‚Äôll want to query a single cell, and search for all the similar cells in the image. For instance, we‚Äôve got this snazzy looking cell of‚Ä¶ dirt over here. Since we have this query‚Äôs similarity scores for every cell, we can rank the cells, filtering those most similar to it. The na√Øve approach is to choose the top-(k) similar cells, but the manual task of determining the optimal number of cells remains. An alternative is to let the image speak for itself: we‚Äôll break down the distribution of scores into a Gaussian mixture model[2] and select cells belonging to the highest-scoring cluster. Doing scienceWe might have managed to produce a visually consistent result, but we‚Äôll also need to design a performance metric, which lets us perform science: tweak parameters and measure how things change. From the final group of similar cells, we can recover a coarse segmentation mask, compared to the original land-cover segmentation mask on the right. Though we‚Äôve managed to match a significant proportion of the cover, we‚Äôve also let too much through. To quantify our closeness to the original segmentation mask, we‚Äôll employ the Jaccard index[3], or intersection-over-union (IoU): $$J(y, \\hat{y}) = \\frac{\\lvert y \\cap \\hat{y} \\rvert}{\\lvert y \\cup \\hat{y} \\rvert} = 0.447$$ That‚Äôs great! Now we‚Äôre able to tweak some design choices and observe how close our results are to the ground-truth segmentation. For instance, we can vary the cell size, and track its effect on the Jaccard index. For each cell size, we‚Äôd take the average IoU over 5 runs, using a random query cell each time. This hints that the optimal cell size for this particular image is around 35px ‚Äì if we‚Äôre restricting ourselves to a fixed cell size, we can run this experiment over many images and try and determine the best average case. ConclusionThis was a useful early approach to distribute objects on underrepresented areas. Though this method retains and leverages the raw power of a trained neural network, it‚Äôs fast and quick to develop, since no training is involved! References 1.Sergey Zagoruyko, &amp; Nikos Komodakis (2015). Learning to Compare Image Patches via Convolutional Neural Networks. arXiv preprint arXiv: Arxiv-1504.03641. ‚Ü©2.Duda, R., &amp; Hart, P. (1973). Pattern classification and scene analysis. (Vol. 3) Wiley New York. ‚Ü©3.Jaccard, P. (1912). THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE. New Phytologist, 11(2), 37-50. ‚Ü©","link":"/2021/12/03/Creating-Synthetic-Data-via-Neural-Placement/"}],"tags":[{"name":"ml","slug":"ml","link":"/tags/ml/"},{"name":"embeddings","slug":"embeddings","link":"/tags/embeddings/"},{"name":"meta","slug":"meta","link":"/tags/meta/"},{"name":"synthetic-data","slug":"synthetic-data","link":"/tags/synthetic-data/"}],"categories":[],"pages":[{"title":"Joel Huang","text":"üëæ About meI grew up in the 90s without much tech, but by the turn of the millennium I was building and publishing blog skins, modding and sharing GameBoy ROMs, and running (legally suspect) private servers for friends to experience games in ways they otherwise wouldn‚Äôt be able to. I started my career developing software and AI, new tools for the same old job - to build stuff people love. This part of me will stay. üßë‚Äçüíª Startup lifeI now lead work on AI and product at Bifrost, a Series A startup building 3D virtual worlds as sandboxes for AI. To put it simply, it‚Äôs building photorealistic 3D worlds (√† la Matrix) and having AI systems train within so they learn what to expect, and having them validate those newfound skills in the real world. Startup work is diverse, by choice. I lead an applied AI team that focuses on generative AI, synthetic data, and data-centric computer vision / MLOps. I also play a leading role in product strategy and discovery, prototyping, AI consultancy and solutions engineering, hiring and teambuilding, content strategy, and more good stuff. More about Bifrost Our goal is to let anyone create realistic 3D worlds easily so they can train and test AI systems properly. Our customers include teams building perception and vision systems across automotive, robotics, defense, intelligence, and more. Technically, our stack is monstrously complex. We had to develop expertise across highly fragmented 3D tooling, build AI models, tools, and data engines to generate and test realistic and performant data, and write custom software for orchestration and scale. We‚Äôre also constantly working on user experience and product design for the specification of 3D worlds and synthetic data development. Bifrost is attemping to solve data, the most important problem in AI, in a time where it matters the most. We are backed by the likes of Sequoia, Wavemaker, Lux, Carbide, and other wonderful investors. üß† Education and workI‚Äôve spent 3 years leading teams and building product and 7 years developing AI systems. I graduated with a Computer Science degree from Singapore University of Technology and Design, and a master‚Äôs degree in AI from the National University of Singapore. Before Bifrost, I worked on computer vision and machine learning research with orgs like Singtel, Nvidia, and NUS. ‚ú® Other things I enjoyBouldering. I recently started my journey, wore out my first pair of shoes in 3 months, and continue to climb ~once a week. Music performance, digital music and sound synthesis. Built software for a gesture-control armband to audiovisually produce music live. Last played keys/synthesizer for Singaporean rapper BGourd and campus band Spacecrumbs. üì± ContactFor professional matters (consulting, partnerships, talks, workshops, hiring, etc) - You‚Äôre welcome to reach out on LinkedIn or Twitter. For personal matters (mentorship, coffee chats), do reach me via Telegram or email.","link":"/index.html"}]}